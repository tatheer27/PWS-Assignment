{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering** ‚≠ê"
      ],
      "metadata": {
        "id": "A8nIjKs0NCOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.**What is a parameter**?\n",
        ">A parameter is a value that you pass into a function, method, or system to customize how it works.parameter is like a placeholder in a function definition. When you call the function, you pass in an argument (the actual value).\n",
        "\n",
        "###2.**What is correlation? What does negative correlation mean**?\n",
        ">Correlation is a statistical measure that describes the relationship between two variables ‚Äî basically, how they move in relation to each other.\n",
        "\n",
        "    It tells you if and how strongly pairs of variables are related.\n",
        "    It's measured with a number called the correlation coefficient (r), which ranges from -1 to +1.\n",
        "\n",
        "###3.**Define Machine Learning. What are the main components in Machine Learning**?\n",
        ">Machine Learning (ML) is a subset of artificial intelligence (AI) that enables systems to automatically learn and improve from experience without being explicitly programmed. In simple terms, it's the science of getting computers to learn patterns from data and make decisions or predictions based on that data.\n",
        "  \n",
        "##The foundation of machine learning.\n",
        "\n",
        "##Includes input features (independent variables) and sometimes labeled output (dependent variables).\n",
        "\n",
        ">Quality and quantity of data significantly impact model performance.\n",
        "\n",
        ">Model (Algorithm)\n",
        "\n",
        ">The mathematical structure or method that learns patterns from the data.\n",
        "\n",
        ">Examples: Linear Regression, Decision Trees, Neural Networks, etc.\n",
        "\n",
        ">Features\n",
        "\n",
        ">The individual measurable properties or characteristics of the data.\n",
        "\n",
        ">Feature selection and engineering are critical to improving model accuracy.\n",
        "\n",
        ">Training\n",
        "\n",
        ">The process where the model learns from the input data.\n",
        "\n",
        ">Involves feeding data to the model and adjusting parameters to minimize error.\n",
        "\n",
        ">Evaluation\n",
        "\n",
        ">Testing the model's performance using unseen data (validation/test set).\n",
        "\n",
        ">Common metrics: Accuracy, Precision, Recall, F1-Score, RMSE, etc.\n",
        "\n",
        ">Prediction / Inference\n",
        "\n",
        ">Once trained, the model is used to make predictions or decisions on new, >unseen data.\n",
        "\n",
        ">Loss Function\n",
        "\n",
        ">A function that measures how well the model‚Äôs predictions match the actual >outcomes.\n",
        "\n",
        ">Used to guide the learning process by minimizing this loss.\n",
        "\n",
        ">Optimizer\n",
        "\n",
        ">An algorithm that adjusts the model‚Äôs parameters to minimize the loss function.\n",
        "\n",
        ">Examples: Gradient Descent, Adam Optimizer.\n",
        "\n",
        "\n",
        "###4.**How does loss value help in determining whether the model is good or not**?\n",
        ">Great question! The loss value is a key metric used during training and evaluation of a machine learning model, and it helps determine how well the model is performing. Here's how it works:\n",
        "\n",
        "##What is a Loss Value?\n",
        ">The loss function measures the difference between the model‚Äôs predictions and the actual target values.\n",
        "\n",
        ">The loss value is the output of that function ‚Äî a single number that tells you how \"wrong\" the model is.\n",
        "\n",
        "##Why Does Loss Matter?\n",
        ">During Training:\n",
        "\n",
        ">The model uses backpropagation to minimize the loss.\n",
        "\n",
        ">A lower loss means the model is making predictions closer to the true labels.\n",
        "\n",
        "##Evaluating Model Performance:\n",
        "\n",
        ">You monitor the training loss (on the training data) and validation loss (on unseen data).\n",
        "\n",
        ">If both are low, the model is likely performing well.\n",
        "\n",
        "###5.**What are continuous and categorical variables**?\n",
        "\n",
        "##Continuous Variables\n",
        ">These are quantitative variables that can take on any value within a range.\n",
        "\n",
        ">They are measurable and can have decimals or fractions.\n",
        "\n",
        ">Think of them as being on a spectrum.\n",
        "\n",
        "##Examples:\n",
        "\n",
        ">Height (e.g., 172.4 cm)\n",
        "\n",
        ">Weight (e.g., 68.2 kg)\n",
        "\n",
        ">Temperature (e.g., 36.6¬∞C)\n",
        "\n",
        ">Time (e.g., 3.14 seconds)\n",
        "\n",
        "##Categorical Variables:\n",
        ">These are qualitative variables that represent groups or categories.\n",
        "\n",
        ">They can‚Äôt be measured numerically in a meaningful way (though sometimes we assign numbers to categories for convenience).\n",
        "\n",
        ">Think of them as labels or types.\n",
        "\n",
        "##Types of Categorical Variables:\n",
        "\n",
        ">Nominal ‚Äì No natural order.\n",
        "\n",
        ">e.g., Colors (Red, Blue, Green), Gender (Male, Female)\n",
        "\n",
        ">Ordinal ‚Äì Has a meaningful order, but differences between values aren‚Äôt consistent.\n",
        "\n",
        ">e.g., Ratings (Poor, Fair, Good, Excellent), Education Level (High School, Bachelor‚Äôs, Master‚Äôs)\n",
        "\n",
        "###6.**How do we handle categorical variables in Machine Learning? What are the common techinque**?\n",
        ">Handling categorical variables in machine learning is a crucial preprocessing step since many algorithms require numerical input. Categorical variables represent data types that can take on a limited number of categories or labels, like color = red/green/blue or city = London/Paris/Tokyo.\n",
        "Here are the common techniques for handling them:\n",
        "\n",
        "üîπ 1. Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "ini\n",
        "Copy\n",
        "Edit\n",
        "color = [red, green, blue] ‚Üí [0, 1, 2]\n",
        "‚úÖ Pros:\n",
        "\n",
        "Simple and memory-efficient.\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "Imposes ordinal relationships (0 < 1 < 2) which may not exist.\n",
        "\n",
        "üìå Use when:\n",
        "\n",
        "The variable is ordinal (e.g., low < medium < high).\n",
        "\n",
        "üîπ 2. One-Hot Encoding\n",
        "Creates a binary column for each category.\n",
        "\n",
        "Example:\n",
        "\n",
        "makefile\n",
        "Copy\n",
        "Edit\n",
        "color = [red, green, blue] ‚Üí\n",
        "red   ‚Üí [1, 0, 0]\n",
        "green ‚Üí [0, 1, 0]\n",
        "blue  ‚Üí [0, 0, 1]\n",
        "‚úÖ Pros:\n",
        "\n",
        "No ordinal relationships assumed.\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "Can lead to high dimensionality (curse of dimensionality).\n",
        "\n",
        "üìå Use when:\n",
        "\n",
        "Categories are nominal (no intrinsic order) and relatively few in number.\n",
        "\n",
        "üîπ 3. Ordinal Encoding\n",
        "Similar to label encoding but used intentionally for ordered categories.\n",
        "\n",
        "Example:\n",
        "\n",
        "ini\n",
        "Copy\n",
        "Edit\n",
        "size = [small, medium, large] ‚Üí [0, 1, 2]\n",
        "‚úÖ Pros:\n",
        "\n",
        "Preserves order.\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "Not suitable for non-ordered data.\n",
        "\n",
        "üîπ 4. Binary Encoding\n",
        "Converts categories into binary format and then splits digits into separate columns.\n",
        "\n",
        "More compact than one-hot.\n",
        "\n",
        "Example (simplified):\n",
        "\n",
        "scss\n",
        "Copy\n",
        "Edit\n",
        "red (1) ‚Üí 001\n",
        "green (2) ‚Üí 010\n",
        "blue (3) ‚Üí 011\n",
        "‚úÖ Pros:\n",
        "\n",
        "Reduces dimensionality compared to one-hot.\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "Not always interpretable.\n",
        "\n",
        "üîπ 5. Target Encoding (Mean Encoding)\n",
        "Replace each category with the mean of the target variable for that category.\n",
        "\n",
        "Example:\n",
        "\n",
        "perl\n",
        "Copy\n",
        "Edit\n",
        "city ‚Üí avg_salary for each city.\n",
        "‚úÖ Pros:\n",
        "\n",
        "Powerful for some models (e.g., tree-based).\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "Can cause data leakage and overfitting if not used carefully (e.g., not using cross-validation).\n",
        "\n",
        "üìå Use with caution and often with regularization.\n",
        "\n",
        "üîπ 6. Frequency / Count Encoding\n",
        "Replace each category with its frequency or count in the dataset.\n",
        "\n",
        "Example:\n",
        "\n",
        "yaml\n",
        "Copy\n",
        "Edit\n",
        "city = [London, Paris, Paris, Tokyo] ‚Üí\n",
        "London: 1, Paris: 2, Tokyo: 1\n",
        "‚úÖ Pros:\n",
        "\n",
        "Keeps numerical simplicity.\n",
        "\n",
        "‚ùå Cons:\n",
        "\n",
        "Loses semantic meaning.\n",
        "\n",
        "Bonus: Advanced / Specialized Methods\n",
        "Embeddings: Useful for high-cardinality categorical variables, especially in deep learning (e.g., word embeddings).\n",
        "\n",
        "Hash Encoding: Good for very high-cardinality features; uses hash functions to map to a fixed number of columns.\n",
        "\n",
        "###7.**What do you mean by training and testing a dataset**?\n",
        ">When we talk about training and testing a dataset in the context of machine learning or data science, we're referring to how we split the data and use it in different stages of building a model.\n",
        "\n",
        "##Training the Dataset\n",
        "This is the part of the dataset the model learns from.\n",
        "\n",
        "You give it examples (input data and the correct answers), and it adjusts its internal settings to improve its predictions.\n",
        "\n",
        "Think of it like studying for an exam: you're learning from books and notes.\n",
        "\n",
        "##Testing the Dataset\n",
        "This is separate from the training data.\n",
        "\n",
        "It's used to evaluate how well the model performs on data it hasn‚Äôt seen before.\n",
        "\n",
        "This helps check if the model is generalizing well (i.e., not just memorizing the training data).\n",
        "\n",
        "It's like taking a practice test after you've studied ‚Äî it tells you how prepared you are.\n",
        "\n",
        "###8.**What is sklearn.preprocessing**?\n",
        ">sklearn.preprocessing is a module in scikit-learn, a popular Python machine learning library. This module provides a variety of tools and classes to preprocess data before feeding it into a machine learning model.\n",
        "\n",
        ">Preprocessing is crucial because raw data often needs to be transformed or normalized to make it suitable for algorithms.\n",
        "\n",
        "###9.**What is a Test set**?\n",
        ">A test set is a subset of data used to evaluate the performance of a machine learning model after it has been trained.\n",
        "\n",
        ">Here‚Äôs how it fits into the typical machine learning workflow:\n",
        "\n",
        ">Dataset Splitting: The full dataset is usually split into three parts:\n",
        "\n",
        "Training set: Used to train the model.\n",
        "\n",
        ">Validation set (optional but common): Used to tune the model and check performance during training.\n",
        "\n",
        "Test set: Used only at the end to assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "Purpose of the Test Set:\n",
        "\n",
        "It helps answer: ‚ÄúHow well will this model perform in the real world?‚Äù\n",
        "\n",
        "It provides an unbiased evaluation because it hasn‚Äôt influenced the model‚Äôs training.\n",
        "\n",
        "Example:\n",
        "Say you have 10,000 data samples. You might split them like this:\n",
        "\n",
        "70% (7,000) ‚Üí training\n",
        "\n",
        "15% (1,500) ‚Üí validation\n",
        "\n",
        "15% (1,500) ‚Üí test\n",
        "\n",
        "##Key Rule:\n",
        "üëâ Never train or tune your model on the test set.\n",
        "It's like the final exam ‚Äî you don't study using the exam questions.\n",
        "\n",
        "Let me know if you want a quick visual or code example for this!\n",
        "\n",
        "###10.**How do we split data for model fitting (training and testing) in Python**?\n",
        ">In Python, a common and efficient way to split data for model fitting (training and testing) is by using.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example data\n",
        "X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]  # Features\n",
        "y = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]                     # Labels/targets\n",
        "\n",
        "# Split data: 80% training, 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train set:\", X_train, y_train)\n",
        "print(\"Test set:\", X_test, y_test)\n",
        "\n",
        ">test_size: Fraction or number of data points to put in the test set (e.g., 0.2 means 20%).\n",
        "\n",
        "train_size: Optionally, you can specify this instead of test_size.\n",
        "\n",
        "random_state: Ensures reproducibility. Set this to any integer for consistent shuffling.\n",
        "\n",
        "shuffle: Whether or not to shuffle the data before splitting (default is True).\n",
        "\n",
        "###**How do you approach a Machine Learning problem**?\n",
        ">Approaching a machine learning (ML) problem is like solving a puzzle with both data and domain understanding. Here‚Äôs a high-level, structured way to tackle it:\n",
        "\n",
        "##üß† 1. Understand the Problem\n",
        "Clarify the objective: Classification, regression, clustering, recommendation, etc.?\n",
        "\n",
        "What‚Äôs the desired output: Predict a label? Estimate a number? Group items?\n",
        "\n",
        "Success metrics: Accuracy, F1 score, RMSE, AUC, etc.\n",
        "\n",
        "##üìä 2. Understand the Data\n",
        "Data collection: Where does the data come from? Is it clean, complete, trustworthy?\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Summary statistics\n",
        "\n",
        "Visualizations (histograms, scatter plots, boxplots)\n",
        "\n",
        "Correlations\n",
        "\n",
        "Check for imbalanced classes, outliers, missing values\n",
        "\n",
        "##üßπ 3. Data Preprocessing\n",
        "Cleaning:\n",
        "\n",
        "Handle missing values (drop, impute)\n",
        "\n",
        "Remove or correct outliers\n",
        "\n",
        "Feature engineering:\n",
        "\n",
        "Encode categorical variables\n",
        "\n",
        "Normalize/standardize numerical features\n",
        "\n",
        "Create interaction or polynomial features\n",
        "\n",
        "Reduce dimensionality if needed (e.g., PCA)\n",
        "\n",
        "Splitting:\n",
        "\n",
        "Train/test split (and optionally a validation set or use cross-validation)\n",
        "\n",
        "##‚öôÔ∏è 4. Model Selection\n",
        "Baseline model: Start simple (e.g., logistic regression, decision tree)\n",
        "\n",
        "Advanced models: Try more complex models (e.g., Random Forest, XGBoost, SVM, Neural Networks)\n",
        "\n",
        "Hyperparameter tuning: Use GridSearchCV, RandomizedSearchCV, or Bayesian Optimization\n",
        "\n",
        "##üß™ 5. Evaluation\n",
        "Use your chosen metric(s) to evaluate:\n",
        "\n",
        "On cross-validation\n",
        "\n",
        "On the test set (only at the end!)\n",
        "\n",
        "Look at confusion matrix, ROC curve, precision-recall, etc.\n",
        "\n",
        "##üì¶ 6. Deployment (if needed)\n",
        "Model serialization: Save the model using joblib, pickle, or frameworks like MLflow\n",
        "\n",
        "API creation: Use Flask/FastAPI for model inference\n",
        "\n",
        "Monitoring: Track performance over time, data drift, retraining needs\n",
        "\n",
        "##üîÅ 7. Iterate\n",
        "Get feedback\n",
        "\n",
        "Improve features\n",
        "\n",
        "Try new models\n",
        "\n",
        "Optimize for speed, memory, or interpretability depending on use case\n",
        "\n",
        "If you want, I can walk you through this approach using a specific example‚Äîjust give me a scenario or dataset you're interested in.\n",
        "\n",
        "###11.**Why do we have to perform EDA before fitting a model to the data**?\n",
        ">Exploratory Data Analysis (EDA) is a crucial first step before fitting a model to your data because it helps you understand the data‚Äôs structure, quality, and underlying patterns. Skipping EDA is like trying to cook a new dish without checking the ingredients first ‚Äî it could work out, but it probably won‚Äôt.\n",
        "\n",
        "##1. Understand the Data\n",
        "You get a feel for what the data looks like: number of features, types of variables (numeric, categorical, datetime, etc.), value ranges, distributions.\n",
        "\n",
        "Helps in identifying what kind of model might be suitable (e.g., linear regression vs. tree-based models).\n",
        "\n",
        "##üö´ 2. Detect Missing or Corrupt Data\n",
        "You can spot nulls, NaNs, or placeholders like -999 or \"Unknown\" that need to be handled.\n",
        "\n",
        "Unhandled missing data can crash your model or skew results.\n",
        "\n",
        "##üìä 3. Visualize Relationships\n",
        "EDA often includes plots (scatter plots, histograms, box plots, etc.) to visualize how variables relate to each other and the target.\n",
        "\n",
        "You might find linear trends, clusters, or outliers that suggest a specific modeling approach or pre-processing step.\n",
        "\n",
        "##‚ö†Ô∏è 4. Spot Outliers or Anomalies\n",
        "Outliers can significantly affect models, especially those that assume normality or are sensitive to extremes (like linear regression).\n",
        "\n",
        "##üîÅ 5. Check for Data Leakage or Feature Redundancy\n",
        "You might find that some features are highly correlated with each other or with the target, which can lead to overfitting or misleading performance.\n",
        "\n",
        "##üß™ 6. Inform Feature Engineering\n",
        "EDA helps generate ideas for transforming variables (e.g., log-transforming skewed data), creating new features, or encoding categoricals in meaningful ways.\n",
        "\n",
        "##üí° Bottom Line:\n",
        "EDA helps you build better models by reducing surprises later on. It gives you the insights needed to clean, prepare, and structure your data in a way that aligns with your modeling goals\n",
        "\n",
        "###12.What is correlation?\n",
        ">Correlation is a statistical measure that describes the relationship between two variables. It shows whether, and how strongly, the variables are related to each other.\n",
        "\n",
        "##Key points:\n",
        "Positive correlation: As one variable increases, the other tends to increase too.\n",
        "Example: Height and weight often have a positive correlation.\n",
        "\n",
        "Negative correlation: As one variable increases, the other tends to decrease.\n",
        "Example: The amount of time spent watching TV and test scores might have a negative correlation.\n",
        "\n",
        "No correlation: There‚Äôs no predictable relationship between the variables.\n",
        "Example: Shoe size and intelligence likely have no correlation\n",
        "\n",
        "###13.**What does negative correlation mean**?\n",
        ">A negative correlation means that as one variable increases, the other decreases. In other words, they move in opposite directions.\n",
        "\n",
        "##Example:\n",
        ">If hours spent watching TV increases, and test scores decrease, those two might have a negative correlation.\n",
        "\n",
        "If temperature goes down, and heating bills go up, that‚Äôs also a negative correlation.\n",
        "\n",
        "In numbers:\n",
        "Correlation is measured with a correlation coefficient, usually called r, which ranges from -1 to 1.\n",
        "\n",
        "r = -1 means a perfect negative correlation.\n",
        "\n",
        "r = 0 means no correlation.\n",
        "\n",
        "r = 1 means a perfect positive correlation\n",
        "\n",
        "###14.**How can you find correlation between variables in Python**?\n",
        ">To find the correlation between variables in Python, you can use libraries like pandas, numpy, or scipy. Here's a quick overview with examples:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7_TnsW6ANt7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample dataframe\n",
        "data = {\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [5, 4, 3, 2, 1],\n",
        "    'C': [2, 3, 4, 5, 6]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Compute correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ac4IrWEcvx_",
        "outputId": "72a720f2-300a-4df3-f406-d385a8470588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     A    B    C\n",
            "A  1.0 -1.0  1.0\n",
            "B -1.0  1.0 -1.0\n",
            "C  1.0 -1.0  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15.**What is causation? Explain difference between correlation and causation with an example**?\n",
        ">Causation refers to a relationship between two variables where one variable directly affects the other. In simple terms, if A causes B, then changes in A bring about changes in B.\n",
        "##Example to Understand the Difference\n",
        ">Let‚Äôs say a study finds that:\n",
        "\n",
        "Ice cream sales and drowning incidents increase at the same time.\n",
        "\n",
        "This is a correlation, not causation.\n",
        "\n",
        ">Why?\n",
        "\n",
        "Eating ice cream doesn‚Äôt cause drowning.\n",
        "\n",
        "The real causal factor is hot weather (a third variable), which leads to both:\n",
        "\n",
        "more people buying ice cream üç¶\n",
        "\n",
        "more people swimming, and unfortunately, more drowning incidents üèä‚Äç‚ôÇÔ∏è\n",
        "\n",
        "So:\n",
        "\n",
        "Correlation: Ice cream sales ‚Üî Drowning\n",
        "\n",
        "Causation: Hot weather ‚Üí Both ice cream sales and swimming\n",
        "\n",
        "###16.**What is an Optimizer? What are different types of optimizers? Explain each with an example**?\n",
        ">An optimizer in machine learning (especially deep learning) is an algorithm or method used to update the weights and biases of a neural network to minimize the loss function. In simpler terms, it's a tool that helps the model learn by improving its performance during training.\n",
        "\n",
        "##What Does an Optimizer Do?\n",
        "The training process involves:\n",
        "\n",
        "Making predictions with current model weights.\n",
        "\n",
        "Calculating how wrong those predictions are (using a loss function).\n",
        "\n",
        "Using the optimizer to adjust the model‚Äôs weights to reduce that loss.\n",
        "\n",
        "##‚öôÔ∏è Types of Optimizers\n",
        "Optimizers fall mainly into two categories:\n",
        "\n",
        "Gradient Descent-Based Optimizers\n",
        "\n",
        "Adaptive Learning Rate Optimizers\n",
        "\n",
        "Let‚Äôs explore the common ones with examples.\n",
        "\n",
        "1. Gradient Descent (GD)\n",
        "‚û§ Vanilla Gradient Descent\n",
        "Updates weights based on the entire training dataset.\n",
        "\n",
        "Not practical for large datasets due to slow speed.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùõº\n",
        "‚àó\n",
        "‚àá\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "Œ∏=Œ∏‚àíŒ±‚àó‚àáJ(Œ∏)\n",
        "Where:\n",
        "\n",
        "Œ∏ = model parameters\n",
        "\n",
        "Œ± = learning rate\n",
        "\n",
        "‚àáJ(Œ∏) = gradient of loss function\n",
        "\n",
        "‚úÖ Example: If loss increases when Œ∏ = 2, then GD will reduce Œ∏ based on gradient direction.\n",
        "\n",
        "##2. Stochastic Gradient Descent (SGD)\n",
        "Updates weights using one data sample at a time.\n",
        "\n",
        "Faster, more noisy.\n",
        "\n",
        "Update Rule: Same as GD, but gradient is computed per sample.\n",
        "\n",
        "‚úÖ Example: For 1000 samples, SGD updates weights 1000 times per epoch (vs. 1 in vanilla GD).\n",
        "\n",
        "##3. Mini-Batch Gradient Descent\n",
        "Compromise between GD and SGD.\n",
        "\n",
        "Uses small batches (e.g., 32 samples) for weight updates.\n",
        "\n",
        "‚úÖ Example: Batch size of 32 on a dataset of 1000 means 1000 / 32 = ~32 updates per epoch.\n",
        "\n",
        "##4. Momentum\n",
        "Speeds up learning by considering past gradients.\n",
        "\n",
        "Helps in faster convergence and avoids local minima.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùë£\n",
        "=\n",
        "ùõΩ\n",
        "ùë£\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        ")\n",
        "‚àá\n",
        "ùêΩ\n",
        "(\n",
        "ùúÉ\n",
        ")\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùõº\n",
        "‚àó\n",
        "ùë£\n",
        "v=Œ≤v+(1‚àíŒ≤)‚àáJ(Œ∏)Œ∏=Œ∏‚àíŒ±‚àóv\n",
        "‚úÖ Example: Imagine a ball rolling downhill‚Äîmomentum helps it build speed and direction.\n",
        "\n",
        "##5. RMSProp\n",
        "Uses exponentially decaying average of squared gradients.\n",
        "\n",
        "Deals well with vanishing/exploding gradients.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùê∏\n",
        "[\n",
        "ùëî\n",
        "2\n",
        "]\n",
        "ùë°\n",
        "=\n",
        "ùõæ\n",
        "ùê∏\n",
        "[\n",
        "ùëî\n",
        "2\n",
        "]\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõæ\n",
        ")\n",
        "ùëî\n",
        "ùë°\n",
        "2\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùõº\n",
        "‚àó\n",
        "ùëî\n",
        "ùë°\n",
        "/\n",
        "(\n",
        "ùë†\n",
        "ùëû\n",
        "ùëü\n",
        "ùë°\n",
        "(\n",
        "ùê∏\n",
        "[\n",
        "ùëî\n",
        "2\n",
        "]\n",
        "ùë°\n",
        ")\n",
        "+\n",
        "ùúÄ\n",
        ")\n",
        "E[g\n",
        "2\n",
        " ]\n",
        "t\n",
        "‚Äã\n",
        " =Œ≥E[g\n",
        "2\n",
        " ]\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≥)g\n",
        "t\n",
        "2\n",
        "‚Äã\n",
        " Œ∏=Œ∏‚àíŒ±‚àóg\n",
        "t\n",
        "‚Äã\n",
        " /(sqrt(E[g\n",
        "2\n",
        " ]\n",
        "t\n",
        "‚Äã\n",
        " )+Œµ)\n",
        "‚úÖ Example: In RNNs or deep networks, RMSProp stabilizes learning by scaling the learning rate adaptively.\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)\n",
        "Combines Momentum and RMSProp.\n",
        "\n",
        "Most widely used optimizer.\n",
        "\n",
        "Maintains separate learning rates for each parameter.\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "ùëö\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "1\n",
        "‚àó\n",
        "ùëö\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "1\n",
        ")\n",
        "‚àó\n",
        "ùëî\n",
        "ùë°\n",
        "ùë£\n",
        "ùë°\n",
        "=\n",
        "ùõΩ\n",
        "2\n",
        "‚àó\n",
        "ùë£\n",
        "ùë°\n",
        "‚àí\n",
        "1\n",
        "+\n",
        "(\n",
        "1\n",
        "‚àí\n",
        "ùõΩ\n",
        "2\n",
        ")\n",
        "‚àó\n",
        "ùëî\n",
        "ùë°\n",
        "2\n",
        "ùúÉ\n",
        "=\n",
        "ùúÉ\n",
        "‚àí\n",
        "ùõº\n",
        "‚àó\n",
        "ùëö\n",
        "ùë°\n",
        "/\n",
        "(\n",
        "ùë†\n",
        "ùëû\n",
        "ùëü\n",
        "ùë°\n",
        "(\n",
        "ùë£\n",
        "ùë°\n",
        ")\n",
        "+\n",
        "ùúÄ\n",
        ")\n",
        "m\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤\n",
        "1\n",
        "‚Äã\n",
        " ‚àóm\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤\n",
        "1\n",
        "‚Äã\n",
        " )‚àóg\n",
        "t\n",
        "‚Äã\n",
        " v\n",
        "t\n",
        "‚Äã\n",
        " =Œ≤\n",
        "2\n",
        "‚Äã\n",
        " ‚àóv\n",
        "t‚àí1\n",
        "‚Äã\n",
        " +(1‚àíŒ≤\n",
        "2\n",
        "‚Äã\n",
        " )‚àóg\n",
        "t\n",
        "2\n",
        "‚Äã\n",
        " Œ∏=Œ∏‚àíŒ±‚àóm\n",
        "t\n",
        "‚Äã\n",
        " /(sqrt(v\n",
        "t\n",
        "‚Äã\n",
        " )+Œµ)\n",
        "‚úÖ Example: Used in models like BERT, GPT, etc., because of its speed and efficiency.\n",
        "\n",
        "###17.**What is sklearn.linear_model**?\n",
        ">sklearn.linear_model is a module in scikit-learn (a popular Python machine learning library) that provides classes and functions for linear models‚Äîthese are models that assume a linear relationship between input variables (features) and the output (target).\n",
        "\n",
        "###18.**What does model.fit() do? What arguments must be given**?\n",
        ">In machine learning libraries like Keras (a high-level API for TensorFlow), model.fit() is a key function used to train a model on a dataset.\n",
        "\n",
        "##‚úÖ What model.fit() does\n",
        "model.fit() trains your neural network model for a fixed number of epochs (iterations over the entire dataset), using the training data and corresponding labels. Under the hood, it:\n",
        "\n",
        "Feeds input data into the model.\n",
        "\n",
        "Calculates the loss between predictions and true labels.\n",
        "\n",
        "Backpropagates the error and updates weights using an optimizer.\n",
        "\n",
        "###19.**What does model.predict() do? What arguments must be given**?\n",
        ">model.predict() is a method used in many machine learning frameworks (like Keras, TensorFlow, Scikit-learn, etc.) to generate predictions from a trained model on new input data.\n",
        "\n",
        "Let‚Äôs break it down by the common libraries:\n",
        "\n",
        "In Keras / TensorFlow (Deep Learning)\n",
        "üìå Purpose:\n",
        "Used to get the model‚Äôs predictions (e.g., class probabilities, regression values) for new input data.\n",
        "\n",
        "‚úÖ Syntax:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "predictions = model.predict(x, batch_size=None, verbose=0)\n",
        "üîë Arguments:\n",
        "x (required): Input data (e.g., NumPy array, Tensor, or a tf.data dataset).\n",
        "\n",
        "batch_size (optional): Number of samples per prediction batch.\n",
        "\n",
        "verbose (optional):\n",
        "\n",
        "0: silent (default)\n",
        "\n",
        "1: progress bar\n",
        "\n",
        "2: one line per epoch\n",
        "\n",
        "üß† Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "##Assume a model has been trained\n",
        "predictions = model.predict(x_test)\n",
        "If it‚Äôs a classification model:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "In Scikit-learn (Classical ML)\n",
        "üìå Purpose:\n",
        "Predict the labels or values for new input data.\n",
        "\n",
        "‚úÖ Syntax:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "predictions = model.predict(X)\n",
        "üîë Arguments:\n",
        "X (required): 2D array-like structure of input features (e.g., NumPy array, pandas DataFrame).\n",
        "\n",
        "üß† Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "###20.**What are continuous and categorical variables**?\n",
        "##Continuous Variables:\n",
        "These are numerical values that can take on any value within a range.\n",
        "\n",
        "They can be measured and often include decimals.\n",
        "\n",
        "Think of them as values on a number line ‚Äî they‚Äôre smooth and infinite (in theory).\n",
        "\n",
        "üß† Examples:\n",
        "Height (e.g., 172.4 cm)\n",
        "\n",
        "Weight (e.g., 65.8 kg)\n",
        "\n",
        "Temperature (e.g., 36.6¬∞C)\n",
        "\n",
        "Time (e.g., 2.53 seconds)\n",
        "\n",
        "##Categorical Variables:\n",
        "These represent categories or groups.\n",
        "\n",
        "They describe qualities or characteristics, not quantities.\n",
        "\n",
        "They can be nominal (no order) or ordinal (ordered).\n",
        "\n",
        "üß† Examples:\n",
        "Nominal (no order): Gender, Color, Type of car\n",
        "\n",
        "###21.**What is feature scaling? How does it help in Machine Learning**?\n",
        ">Feature scaling is a technique used in machine learning to normalize the range of independent variables (or features) in your data. The main goal is to ensure that all features contribute equally to the result, especially for models that rely on distances or gradients.\n",
        "\n",
        "##üîç What Is Feature Scaling?\n",
        "Feature scaling transforms your data so that each feature falls within a similar scale. Two common methods:\n",
        "\n",
        "Min-Max Scaling (Normalization):\n",
        "\n",
        "Scales values to a fixed range, usually [0, 1].\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "‚Ä≤\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "‚Å°\n",
        "ùëã\n",
        "max\n",
        "‚Å°\n",
        "‚àí\n",
        "ùëã\n",
        "min\n",
        "‚Å°\n",
        "X\n",
        "‚Ä≤\n",
        " =\n",
        "X\n",
        "max\n",
        "‚Äã\n",
        " ‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "X‚àíX\n",
        "min\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "\n",
        "Standardization (Z-score Normalization):\n",
        "\n",
        "Centers the data around 0 with a standard deviation of 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "ùëã\n",
        "‚Ä≤\n",
        "=\n",
        "ùëã\n",
        "‚àí\n",
        "ùúá\n",
        "ùúé\n",
        "X\n",
        "‚Ä≤\n",
        " =\n",
        "œÉ\n",
        "X‚àíŒº\n",
        "‚Äã\n",
        "\n",
        "##‚öôÔ∏è Why Feature Scaling Matters in ML\n",
        "Gradient Descent Optimization:\n",
        "\n",
        "Models like logistic regression or neural networks use gradient descent. If features are on different scales, gradients can become unstable and convergence becomes slower or harder.\n",
        "\n",
        "##Distance-Based Algorithms:\n",
        "\n",
        "Algorithms like KNN, K-means, and SVM rely on distances. If one feature has a much larger scale than others, it will dominate the distance calculation.\n",
        "\n",
        "##Regularization:\n",
        "\n",
        "In regularized models (like Lasso or Ridge regression), feature scale impacts the penalty term. Without scaling, regularization could unfairly penalize some features.\n",
        "\n",
        "Faster Training:\n",
        "\n",
        "Scaling often speeds up the training process and leads to better performance.\n",
        "\n",
        "###22.**How do we perform scaling in Python**?\n",
        ">In Python, scaling typically refers to adjusting the range of data, and it's commonly used in data preprocessing for machine learning. The most popular way to scale data is using scikit-learn. Here are the most common methods:\n",
        "\n",
        "##1. Min-Max Scaling (Normalization)\n",
        "Scales data to a specific range, usually [0, 1].\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "data = [[1], [2], [3], [4], [5]]\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "##2. Standardization (Z-score Scaling)\n",
        "Scales data to have mean = 0 and standard deviation = 1.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = [[1], [2], [3], [4], [5]]\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "##3. Robust Scaling\n",
        "Uses the median and interquartile range ‚Äì useful when data contains outliers.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "data = [[1], [2], [3], [4], [100]]\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "##4. MaxAbs Scaling\n",
        "Scales data to the range [-1, 1] based on the absolute maximum value.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "data = [[1], [2], [3], [4], [5]]\n",
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n",
        "##5. Manual Scaling with NumPy\n",
        "If you're not using scikit-learn:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([1, 2, 3, 4, 5])\n",
        "scaled_data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "print(scaled_data).\n",
        "\n",
        "###23.**What is sklearn.preprocessing**?\n",
        ">sklearn.preprocessing is a module in the Scikit-learn library that provides tools to preprocess or transform your data before feeding it into a machine learning model.\n",
        "\n",
        "Data preprocessing is crucial because many ML algorithms expect the input features to be normalized, scaled, encoded, or otherwise prepared.\n",
        "\n",
        "##üîß Commonly Used Functions in sklearn.preprocessing\n",
        "Here are some of the most useful classes and functions in this module:\n",
        "\n",
        "##1. Scaling / Normalizing\n",
        "StandardScaler\n",
        "Scales features to have zero mean and unit variance.\n",
        "üìà Useful for algorithms that assume data is normally distributed (e.g., Logistic Regression, SVMs).\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "MinMaxScaler\n",
        "Scales features to a given range, typically between 0 and 1.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "Normalizer\n",
        "Scales individual samples to unit norm (row-wise normalization).\n",
        "\n",
        "##‚úÖ 2. Encoding Categorical Variables\n",
        "OneHotEncoder\n",
        "Converts categorical variables into one-hot encoded arrays.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder()\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "LabelEncoder\n",
        "Converts class labels to integers.\n",
        "‚ö†Ô∏è Use only on target labels, not features.\n",
        "\n",
        "##‚úÖ 3. Binarization / Polynomial Features / Other Transforms\n",
        "Binarizer\n",
        "Converts numerical values into binary values based on a threshold.\n",
        "\n",
        "PolynomialFeatures\n",
        "Generates polynomial and interaction features.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "FunctionTransformer\n",
        "Lets you apply custom transformations using Python functions.\n",
        "\n",
        "##‚úÖ 4. Imputation (handling missing values)\n",
        "Although technically in sklearn.impute, it's often used alongside preprocessing:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "###24.**How do we split data for model fitting (training and testing) in Python**?\n",
        ">o split data for model fitting (into training and testing sets) in Python, you typically use train_test_split from the sklearn.model_selection module. Here's how you do it:\n",
        ">from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose you have your features (X) and target labels (y)\n",
        "# X = features, usually a DataFrame or 2D array\n",
        "# y = labels/targets, usually a Series or 1D array\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42).\n",
        "\n",
        "###25.Explain data encoding?\n",
        ">Data encoding is the process of converting data into a specific format so it can be stored, transmitted, or processed efficiently and accurately. It's like translating information into a language that computers or communication systems can understand.\n",
        "\n",
        "##Why is Data Encoding Important?\n",
        "Storage: Encoded data can be compressed to save space.\n",
        "\n",
        "Transmission: Encoding ensures data can be safely sent over networks.\n",
        "\n",
        "Security: Some encoding methods help protect data (e.g., base64 in email attachments).\n",
        "\n",
        "Compatibility: Encoding makes data usable across different systems and software.\n",
        "\n",
        "##Types of Data Encoding\n",
        "Here are some common types, depending on the context:\n",
        "\n",
        "##1.Character Encoding\n",
        "Used to represent text in computers.\n",
        "\n",
        "ASCII ‚Äì Represents English characters with 7 bits.\n",
        "\n",
        "UTF-8 ‚Äì A variable-length encoding for all Unicode characters (most widely used today).\n",
        "\n",
        "UTF-16/UTF-32 ‚Äì Other Unicode encodings with different byte lengths.\n",
        "\n",
        "##. Binary Encoding\n",
        "Translates data (like numbers or files) into binary (0s and 1s) for computer processing.\n",
        "\n",
        "##3. Base Encoding (e.g., Base64)\n",
        "Used to encode binary data (like images or files) into text so it can be easily sent in systems that handle only text, such as emails.\n",
        "\n",
        "##4. Source Encoding (in Communications)\n",
        "Transforms data before sending it over a communication channel.\n",
        "\n",
        "Example: Huffman coding or Run-Length Encoding (RLE) to compress data.\n",
        "\n",
        "##5. Line Encoding (in Networking)\n",
        "Defines how bits are represented on a physical medium (e.g., voltage changes on a wire).\n",
        "\n",
        "##Examples: NRZ, Manchester encoding.\n",
        "\n",
        "üìä Example: Character Encoding\n",
        "Text: \"A\"\n",
        "\n",
        "In ASCII, \"A\" = 65 in decimal = 01000001 in binary.\n",
        "\n",
        "In UTF-8, it's the same because \"A\" is in the basic Latin set.\n",
        "\n",
        "üß† Key Takeaway\n",
        "Data encoding is like choosing the right outfit for data‚Äîit dresses the data in a for.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VQuSizjIc2l9"
      }
    }
  ]
}