{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression Assignment Theory⭐"
      ],
      "metadata": {
        "id": "R_9s_TWu_Ehx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.What is Logistic Regression, and how does it differ from Linear Regression?\n",
        ">Logistic Regression and Linear Regression are both supervised learning algorithms used in machine learning and statistics, but they are used for different types of problems and have different underlying models. Here's a breakdown:\n",
        "Used for predicting continuous outcomes.\n",
        "\n",
        ">Example: Predicting house prices, temperature, or stock prices.\n",
        "\n",
        ">Logistic Regression:\n",
        "\n",
        ">Used for predicting categorical outcomes, typically binary classification (e.>g., 0 or 1, yes or no, spam or not spam).\n",
        "\n",
        ">Can be extended to multiclass classification with techniques like One-vs-Rest.\n",
        "\n",
        ">Output Linear Regression:\n",
        ">Predicts a real-valued number.\n",
        "\n",
        ">Output is unbounded (can be any value from − ∞ −∞ to + ∞ +∞).\n",
        "\n",
        ">Logistic Regression:\n",
        "\n",
        ">Predicts a probability that the input belongs to a certain class.\n",
        "\n",
        ">Output is bounded between 0 and 1 (via the sigmoid function).\n",
        "\n",
        ">Mathematical Form Linear Regression:\n",
        ">𝑦 = 𝛽 0 + 𝛽 1 𝑥 1 + 𝛽 2 𝑥 2 + ⋯ + 𝛽 𝑛 𝑥 𝑛 y=β 0​+β 1​x 1​+β 2​x 2​+⋯>+β n​x n​\n",
        "\n",
        ">Logistic Regression:\n",
        "\n",
        ">𝑝 = 1 1 + 𝑒 − ( 𝛽 0 + 𝛽 1 𝑥 1 + 𝛽 2 𝑥 2 + ⋯ + 𝛽 𝑛 𝑥 𝑛 ) p= 1+e −(β >0​+β 1​x 1​+β 2​x 2​+⋯+β n​x n​)\n",
        "\n",
        ">1​\n",
        "\n",
        ">Where 𝑝 p is the probability that the output is class 1.\n",
        "\n",
        ">Loss Function Linear Regression: Uses Mean Squared Error (MSE).\n",
        ">Logistic Regression: Uses Log Loss (Cross-Entropy Loss).\n",
        "\n",
        ">Interpretation of Coefficients Linear Regression: Coefficients represent the change in the output for a one-unit change in the input.\n",
        "Logistic Regression: Coefficients represent the log-odds of the outcome for a one-unit change in the input.\n",
        "\n",
        ">Application Examples Linear Regression:\n",
        "Predicting salary based on years of experience.\n",
        "\n",
        ">Estimating sales based on advertising budget.\n",
        "\n",
        ">Logistic Regression:\n",
        "\n",
        "###2.What is the mathematical equation of Logistic Regression?\n",
        ">The mathematical equation of Logistic Regression is based on the logistic (sigmoid) function, which models the probability that a given input belongs to a certain class (usually binary classification: 0 or 1).\n",
        "\n",
        ">Logistic (Sigmoid) Function The core of logistic regression is the sigmoid function:\n",
        "𝜎 ( 𝑧 ) = 1 1 + 𝑒 − 𝑧 σ(z)= 1+e −z\n",
        "\n",
        ">This function maps any real-valued number to the range ( 0 , 1 ) (0,1), which can be interpreted as a probability.\n",
        "\n",
        "###3.Why do we use the Sigmoid function in Logistic Regression?\n",
        ">We use the sigmoid function in logistic regression because it transforms the linear output of a model into a probability, which is essential for binary classification tasks.\n",
        "\n",
        ">Here's a breakdown of why it's used:\n",
        "\n",
        ">Converts Any Real Value to a Probability The sigmoid function is defined as:\n",
        "𝜎 ( 𝑧 ) = 1 1 + 𝑒 − 𝑧 σ(z)= 1+e −z\n",
        "\n",
        ">Where:\n",
        "\n",
        ">𝑧 z is the linear combination of input features and weights (i.e., 𝑧 = 𝑤 𝑇 𝑥 + 𝑏 z=w T x+b).\n",
        "\n",
        ">This function maps any real number to a value between 0 and 1, which can be interpreted as the probability of the positive class (e.g., probability of \"yes\", or \"class 1\").\n",
        "\n",
        "###4.What is the cost function of Logistic Regression?\n",
        ">The cost function of Logistic Regression is based on the log loss, also known as the binary cross-entropy loss. It measures how well the predicted probabilities match the actual class labels (0 or 1).\n",
        "\n",
        ">Logistic Regression Cost Function Given:\n",
        "\n",
        ">𝑚 m: number of training examples\n",
        "\n",
        ">𝑦 ( 𝑖 ) y (i) : actual label (0 or 1) for the 𝑖 i-th example\n",
        "\n",
        ">𝑦 ^ ( 𝑖 ) = ℎ 𝜃 ( 𝑥 ( 𝑖 ) ) = 1 1 + 𝑒 − 𝜃 𝑇 𝑥 ( 𝑖 ) y ^​\n",
        "\n",
        ">(i) =h θ​(x (i) )= 1+e −θ T x (i)\n",
        "\n",
        ">1​: predicted probability (sigmoid output)\n",
        "\n",
        ">The cost function 𝐽 ( 𝜃 ) J(θ) is:\n",
        "\n",
        ">𝐽 ( 𝜃 ) = − 1 𝑚 ∑ 𝑖 = 1 𝑚 [ 𝑦 ( 𝑖 ) log ⁡ ( 𝑦 ^ ( 𝑖 ) ) + ( 1 − 𝑦 ( >𝑖 ) ) log ⁡ ( 1 − 𝑦 ^ ( 𝑖 ) ) ] J(θ)=− m 1​\n",
        "\n",
        ">i=1 ∑ m​[y (i) log( y ^​\n",
        "\n",
        ">(i) )+(1−y (i) )log(1− y ^​\n",
        "\n",
        ">(i) )\n",
        "\n",
        "###5.What is Regularization in Logistic Regression? Why is it needed?\n",
        ">Regularization in logistic regression is a technique used to prevent overfitting by penalizing large coefficients in the model. It modifies the cost function to include an additional term that discourages the model from fitting the training data too closely, which can hurt its performance on unseen dat\n",
        "\n",
        ">Why Is it need:In logistic regression (or any model), overfitting happens when:\n",
        "\n",
        ">The model becomes too complex (e.g., too many features or too large coefficients).\n",
        "\n",
        ">It captures noise instead of the underlying pattern.\n",
        "\n",
        ">It performs well on training data but poorly on new/unseen data.\n",
        "\n",
        ">Regularization helps by constraining the optimization process:\n",
        "\n",
        ">It keeps the learned weights (coefficients) small.\n",
        "\n",
        ">This encourages the model to focus on the most important patterns.\n",
        "\n",
        "###6.C Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        ">Lasso, Ridge, and Elastic Net are regularized regression techniques used to prevent overfitting and improve model generalization, especially when dealing with high-dimensional data (many features). Here's how they differ:\n",
        "Ridge Regression (L2 Regularization)\n",
        "Penalty Term: Adds the sum of squared coefficients (L2 norm) to the loss function.\n",
        ">Loss = RSS + 𝜆 ∑ 𝑗 = 1 𝑝 𝛽 𝑗 2 Loss=RSS+λ j=1 ∑ p​β j 2​\n",
        "\n",
        ">Effect: Shrinks coefficients towards zero but does not set them exactly to zero.\n",
        "\n",
        ">Use Case: Best when all features are relevant and multicollinearity exists.\n",
        "\n",
        "Pros:\n",
        "\n",
        ">Works well when many small/medium-sized effects are present.\n",
        "\n",
        ">Stabilizes coefficients in the presence of multicollinearity.\n",
        "\n",
        ">Lasso Regression (L1 Regularization) Penalty Term: Adds the sum of the absolute values of the coefficients (L1 norm).\n",
        "Loss = RSS + 𝜆 ∑ 𝑗 = 1 𝑝 ∣ 𝛽 𝑗 ∣ Loss=RSS+λ j=1 ∑ p​∣β j​∣ Effect: Can shrink some coefficients exactly to zero, thus performing feature selection.\n",
        "\n",
        ">Use Case: Best when you expect only a few features to be important.\n",
        "\n",
        "\n",
        ">Performs variable selection.\n",
        "\n",
        ">Produces sparse models, which are easier to interpret.\n",
        "\n",
        ">Elastic Net Regression Penalty Term: A combination of L1 and L2 regularization.\n",
        ">Loss = RSS + 𝜆 1 ∑ 𝑗 = 1 𝑝 ∣ 𝛽 𝑗 ∣ + 𝜆 2 ∑ 𝑗 = 1 𝑝 𝛽 𝑗 2 Loss=RSS+λ 1​\n",
        "\n",
        ">j=1 ∑ p​∣β j​∣+λ 2​\n",
        "\n",
        ">j=1 ∑ p​β j 2​\n",
        "\n",
        ">Often expressed as:\n",
        "\n",
        ">Loss = RSS + 𝜆 [ 𝛼 ∑ ∣ 𝛽 𝑗 ∣ + ( 1 − 𝛼 ) ∑ 𝛽 𝑗 2 ] Loss=RSS+λ[α∑∣β j​∣+(1−α)∑β j 2​] where 𝛼 ∈ [ 0 , 1 ] α∈[0,1] balances L1 and L2.\n",
        "\n",
        ">Effect: Combines the sparsity of Lasso with the stability of Ridge.\n",
        "\n",
        ">Use Case: Best when there are correlated features or when the number of predictors exceeds the number of observations.\n",
        "\n",
        "\n",
        ">Handles multicollinearity.\n",
        "\n",
        ">Can select groups of correlated variables.\n",
        "\n",
        "\n",
        "###7.When should we use Elastic Net instead of Lasso or Ridge?\n",
        ">Elastic Net is a regularization technique that combines both Lasso (L1) and Ridge (L2) penalties. You should consider using Elastic Net instead of Lasso or Ridge alone in the following situations:\n",
        "\n",
        ">✅ When to Use Elastic Net\n",
        ">High-dimensional data (p > n)\n",
        "\n",
        ">If the number of features p is greater than the number of observations n, Elastic Net often performs better than either Ridge or Lasso alone.\n",
        "\n",
        ">Lasso may randomly select one feature from a group of correlated features and ignore the rest, which can be unstable. Elastic Net tends to select groups of correlated variables together.\n",
        "\n",
        ">Correlated predictors (multicollinearity)\n",
        "\n",
        ">Lasso struggles when predictors are highly correlated—it tends to arbitrarily select one and drop the others.\n",
        "\n",
        ">Ridge shrinks coefficients but does not perform variable selection.\n",
        "\n",
        ">Elastic Net does both: it shrinks like Ridge and performs selection like Lasso, but more robustly handles correlated features.\n",
        "\n",
        ">You want a compromise between sparsity and shrinkage\n",
        "\n",
        ">Lasso gives you sparse solutions (i.e., it sets many coefficients to zero), while Ridge gives small, non-zero coefficients.\n",
        "\n",
        ">Elastic Net allows you to tune the balance between the two with the mixing parameter α:\n",
        "\n",
        ">α = 1: Lasso\n",
        "\n",
        ">α = 0: Ridge\n",
        "\n",
        ">0 < α < 1: Elastic Net (compromise)\n",
        "\n",
        "###8.What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        ">The regularization parameter 𝜆 λ (often denoted as C in scikit-learn, where 𝐶 = 1 𝜆 C= λ 1​) in Logistic Regression controls the amount of regularization applied to the model. Regularization is a technique used to prevent overfitting by penalizing large coefficients in the model.\n",
        "\n",
        ">🔍 Impact of 𝜆 λ in Logistic Regression:\n",
        "\n",
        ">When 𝜆 λ is Small (close to 0): Weak regularization (or none at all).\n",
        "The model can fit the training data very closely.\n",
        "\n",
        ">Risk of overfitting increases.\n",
        "\n",
        ">Coefficients can become large in magnitude.\n",
        "\n",
        ">When 𝜆 λ is Large: Strong regularization is applied.\n",
        "The model is constrained, and coefficients are pushed towards zero.\n",
        "\n",
        ">Helps prevent overfitting by simplifying the model.\n",
        "\n",
        ">May lead to underfitting if too strong, reducing model performance on both training and test data.\n",
        "\n",
        "###9.What are the key assumptions of Logistic Regression?\n",
        ">Logistic Regression is a widely used classification algorithm that models the probability of a binary outcome. While it is less restrictive than linear regression in some ways, it still relies on several important assumptions for valid inference and good performance:\n",
        "\n",
        ">Key Assumptions of Logistic Regression Binary or Categorical Dependent Variable\n",
        "\n",
        ">The dependent variable should be binary (0 or 1) for binary logistic regression. For multinomial or ordinal outcomes, extensions like multinomial or ordinal logistic regression are used.\n",
        "\n",
        ">Linearity of Logits\n",
        "\n",
        ">Logistic regression assumes a linear relationship between the logit (log-odds) of the outcome and the independent variables:\n",
        "\n",
        ">log ⁡ ( 𝑝 1 − 𝑝 ) = 𝛽 0 + 𝛽 1 𝑋 1 + ⋯ + 𝛽 𝑘 𝑋 𝑘 log( 1−p p​)=β 0​+β 1​X 1​+⋯+β k​X k​\n",
        "\n",
        ">Note: This is not a linear relationship between the predictors and the outcome itself, but between predictors and the log-odds of the outcome.\n",
        "\n",
        ">Independence of Observations\n",
        "\n",
        ">Each observation should be independent of the others. This means no repeated measurements or time-series dependencies unless appropriately modeled.\n",
        "\n",
        ">No or Little Multicollinearity\n",
        "\n",
        ">Independent variables should not be highly correlated with each other. High multicollinearity can inflate the variance of coefficient estimates.\n",
        "\n",
        ">Large Sample Size\n",
        "\n",
        ">Logistic regression relies on maximum likelihood estimation (MLE), which performs better with larger sample sizes, especially when the outcome is rare.\n",
        "\n",
        ">No Extreme Outliers in Predictors\n",
        "\n",
        ">Although logistic regression is less sensitive to outliers in the response, outliers in the independent variables can still affect the model.\n",
        "\n",
        ">Predictor Variables Can Be Continuous or Categorical\n",
        "\n",
        ">Logistic regression handles both types of predictors, but categorical variables should be properly encoded (e.g., one-hot or dummy encoding).\n",
        "\n",
        "###10.What are some alternatives to Logistic Regression for classification tasks?\n",
        ">There are many alternatives to Logistic Regression for classification tasks, each with its strengths depending on the dataset characteristics (size, linearity, noise, etc.). Here are some widely used alternatives:\n",
        "\n",
        "1. Support Vector Machines (SVM)\n",
        "Strengths: Works well for both linear and non-linear data (using kernels), effective in high-dimensional spaces.\n",
        "\n",
        "Weaknesses: Computationally intensive for large datasets, harder to interpret.\n",
        "\n",
        "2. Decision Trees\n",
        "Strengths: Simple to interpret, handles both numerical and categorical data, captures non-linear relationships.\n",
        "\n",
        "Weaknesses: Prone to overfitting without pruning or constraints.\n",
        "\n",
        "3. Random Forest\n",
        "Strengths: Ensemble of decision trees, reduces overfitting, handles missing values and unbalanced data well.\n",
        "\n",
        "Weaknesses: Less interpretable than a single decision tree.\n",
        "\n",
        "4. Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost)\n",
        "Strengths: High accuracy, handles complex patterns and non-linearity, often the go-to for structured data.\n",
        "\n",
        "Weaknesses: Requires careful tuning, slower training time.\n",
        "\n",
        "5. k-Nearest Neighbors (k-NN)\n",
        "Strengths: Simple and intuitive, no training phase.\n",
        "\n",
        "Weaknesses: Computationally expensive at prediction time, sensitive to irrelevant features and scaling.\n",
        "\n",
        "6. Naive Bayes\n",
        "Strengths: Fast, works well with text data and high-dimensional features.\n",
        "\n",
        "Weaknesses: Assumes feature independence, which often doesn't hold.\n",
        "\n",
        "7. Neural Networks\n",
        "Strengths: Highly flexible, can model very complex relationships.\n",
        "\n",
        "Weaknesses: Requires more data, prone to overfitting, less interpretable, computationally intensive.\n",
        "\n",
        "8. Linear Discriminant Analysis (LDA)\n",
        "Strengths: Good for linearly separable data, interpretable, works well with small sample sizes.\n",
        "\n",
        "Weaknesses: Assumes normality and equal covariance among classes.\n",
        "\n",
        "9. Quadratic Discriminant Analysis (QDA)\n",
        "Similar to LDA but allows for class-specific covariance matrices, making it more flexible but also more data-hungry.\n",
        "\n",
        "10. Rule-based Classifiers (e.g., RIPPER, PART)\n",
        "Strengths: Easy to interpret and explain.\n",
        "\n",
        "Weaknesses: Less powerful for complex patterns.\n",
        "\n",
        "\n",
        "###11.What are Classification Evaluation Metrics?\n",
        "Classification evaluation metrics are methods used to measure how well a classification model performs. They help you understand how accurately the model predicts the correct class labels. Since classification problems involve assigning inputs into categories, these metrics evaluate the quality of those predictions compared to the actual labels.\n",
        "\n",
        "Here are some of the key classification evaluation metrics:\n",
        "\n",
        "Accuracy Definition: The ratio of correctly predicted instances to the total instances.\n",
        "Formula:\n",
        "\n",
        "Accuracy = Number of correct predictions Total number of predictions Accuracy= Total number of predictions Number of correct predictions​\n",
        "\n",
        "When to use: Good when classes are balanced.\n",
        "\n",
        "Precision Definition: The ratio of correctly predicted positive observations to the total predicted positives.\n",
        "Formula:\n",
        "\n",
        "Precision = 𝑇 𝑃 𝑇 𝑃 + 𝐹 𝑃 Precision= TP+FP TP​\n",
        "\n",
        "Meaning: How many selected items are relevant.\n",
        "\n",
        "Use case: Important when the cost of false positives is high.\n",
        "\n",
        "Recall (Sensitivity or True Positive Rate) Definition: The ratio of correctly predicted positive observations to all actual positives.\n",
        "Formula:\n",
        "\n",
        "Recall = 𝑇 𝑃 𝑇 𝑃 + 𝐹 𝑁 Recall= TP+FN TP​\n",
        "\n",
        "Meaning: How many relevant items are selected.\n",
        "\n",
        "Use case: Important when the cost of false negatives is high.\n",
        "\n",
        "F1 Score Definition: The harmonic mean of Precision and Recall.\n",
        "Formula:\n",
        "\n",
        "𝐹 1 = 2 × Precision × Recall Precision + Recall F1=2× Precision+Recall Precision×Recall​\n",
        "\n",
        "Meaning: Balance between Precision and Recall.\n",
        "\n",
        "Use case: Useful when you want to balance false positives and false negatives.\n",
        "\n",
        "Confusion Matrix A table showing the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n",
        "Predicted Positive Predicted Negative Actual Positive TP FN Actual Negative FP TN\n",
        "\n",
        "Specificity (True Negative Rate) Definition: The ratio of correctly predicted negatives to all actual negatives.\n",
        "Formula:\n",
        "\n",
        "Specificity = 𝑇 𝑁 𝑇 𝑁 + 𝐹 𝑃 Specificity= TN+FP TN​\n",
        "\n",
        "ROC Curve and AUC (Area Under the Curve) ROC Curve: Plots True Positive Rate (Recall) vs. False Positive Rate.\n",
        "AUC: Measures the entire two-dimensional area underneath the ROC curve. A higher AUC indicates better performance.\n",
        "\n",
        "###12.How does class imbalance affect Logistic Regression?\n",
        "Class imbalance can significantly affect the performance of Logistic Regression and other classification models. Here's how:\n",
        "\n",
        "🔍 What is Class Imbalance?\n",
        "Class imbalance occurs when the number of instances in one class far exceeds those in another. For example, in a binary classification problem:\n",
        "\n",
        "Class 0: 95%\n",
        "\n",
        "Class 1: 5%\n",
        "\n",
        "###13.What is Hyperparameter Tuning in Logistic Regression?\n",
        "Hyperparameter tuning in logistic regression refers to the process of selecting the best set of hyperparameters that optimize the model's performance.\n",
        "\n",
        "🔍 What are Hyperparameters?\n",
        "Hyperparameters are settings that are not learned from the data but are set before training the model. In logistic regression, common hyperparameters include:\n",
        "\n",
        "Regularization type (penalty)\n",
        "\n",
        "Options: 'l1', 'l2', 'elasticnet', 'none'\n",
        "\n",
        "Purpose: Prevent overfitting by adding a penalty for large coefficients.\n",
        "\n",
        "Regularization strength (C)\n",
        "\n",
        "Inverse of regularization strength.\n",
        "\n",
        "Lower C → stronger regularization.\n",
        "\n",
        "Default: C=1.0\n",
        "\n",
        "Solver\n",
        "\n",
        "Algorithms used to optimize the cost function.\n",
        "\n",
        "Options: 'liblinear', 'saga', 'lbfgs', 'newton-cg'\n",
        "\n",
        "Some solvers support specific penalties only.\n",
        "\n",
        "Maximum number of iterations (max_iter)\n",
        "\n",
        "Sets how long the solver should keep optimizing.\n",
        "\n",
        "###14.What are different solvers in Logistic Regression? Which one should be used?\n",
        "In logistic regression, solvers are algorithms used to optimize the cost function — that is, to find the best model parameters (weights). Different solvers have different strengths depending on the dataset size, sparsity, and presence of regularization.\n",
        "Which Solver Should You Use?\n",
        "General Guidelines:\n",
        "Small datasets:\n",
        "\n",
        "Use liblinear — fast and reliable for small-scale problems.\n",
        "\n",
        "Large datasets:\n",
        "\n",
        "Use lbfgs, sag, or saga.\n",
        "\n",
        "sag and saga are optimized for large datasets with many samples.\n",
        "\n",
        "L1 regularization (sparse models):\n",
        "\n",
        "Use liblinear or saga.\n",
        "\n",
        "Multinomial classification (not just OvR):\n",
        "\n",
        "Use lbfgs, newton-cg, or saga with multi_class='multinomial'.\n",
        "\n",
        "Sparse data (like text data with TF-IDF):\n",
        "\n",
        "Prefer saga or sag.\n",
        "\n",
        "Recommendation:\n",
        "If unsure and you're not using L1 regularization:\n",
        "Start with lbfgs, as it's robust and works well in most situations.\n",
        "\n",
        "If you need L1 regularization, prefer saga for larger datasets, or liblinear for smaller ones.\n",
        "\n",
        "###15.How is Logistic Regression extended for multiclass classification?\n",
        "Logistic regression is inherently a binary classifier, but it can be extended to handle multiclass classification through several strategies. The most common extensions are:\n",
        "\n",
        "1. One-vs-Rest (OvR) / One-vs-All (OvA)\n",
        "Concept: Train one binary classifier per class.\n",
        "\n",
        "Each classifier predicts whether a sample belongs to its class or not (i.e., \"class k vs. all others\").\n",
        "\n",
        "For K classes, K separate logistic regression models are trained.\n",
        "\n",
        "At prediction time:\n",
        "\n",
        "Each model outputs a probability.\n",
        "\n",
        "The class with the highest probability is selected.\n",
        "\n",
        "✅ Pros: Simple and works well in practice.\n",
        "⚠️ Cons: Can be inefficient with many classes; classifiers may be imbalanced.\n",
        "\n",
        "###16.What are the advantages and disadvantages of Logistic Regression?\n",
        "Logistic Regression is a widely used statistical method for binary classification problems (predicting one of two possible outcomes). Here’s a breakdown of its advantages and disadvantages:\n",
        "\n",
        "✅ Advantages of Logistic Regression\n",
        "Simplicity and Interpretability\n",
        "\n",
        "Easy to implement and interpret.\n",
        "\n",
        "Coefficients provide insight into the relationship between independent variables and the outcome.\n",
        "\n",
        "Computational Efficiency\n",
        "\n",
        "Requires less computational resources compared to more complex models like Random Forests or Neural Networks.\n",
        "\n",
        "Probabilistic Output\n",
        "\n",
        "Produces probabilities for class membership, not just classifications, which is useful for ranking or thresholding.\n",
        "\n",
        "Good Baseline\n",
        "\n",
        "Often used as a baseline model because of its simplicity and robustness.\n",
        "\n",
        "Works Well with Linearly Separable Data\n",
        "\n",
        "Performs well when the data is linearly separable or nearly so.\n",
        "\n",
        "No Need for Feature Scaling (in many cases)\n",
        "\n",
        "Unlike models like SVM or KNN, it doesn’t always require feature scaling (although scaling can still improve performance).\n",
        "\n",
        "Regularization\n",
        "\n",
        "Can be easily extended with L1 (Lasso) or L2 (Ridge) regularization to avoid overfitting.\n",
        "\n",
        "###17.What are some use cases of Logistic Regression?\n",
        "Logistic Regression is a widely used statistical method for binary classification problems, but it also extends to multi-class classification. Here are some key use cases across different domains:\n",
        "\n",
        "🔍 1. Binary Classification Tasks\n",
        "These are the most common applications of logistic regression:\n",
        "\n",
        "Spam Detection\n",
        "Predict whether an email is spam (1) or not (0).\n",
        "\n",
        "Disease Diagnosis\n",
        "Predict if a patient has a disease (e.g., diabetes, cancer) based on medical indicators.\n",
        "\n",
        "Customer Churn Prediction\n",
        "Determine whether a customer will leave a service or stay.\n",
        "\n",
        "Loan Default Prediction\n",
        "Predict if a borrower will default on a loan.\n",
        "\n",
        "Click-Through Rate Prediction\n",
        "Estimate the probability of a user clicking on an advertisement.\n",
        "\n",
        "###18.What is the difference between Softmax Regression and Logistic Regression?\n",
        "Softmax Regression and Logistic Regression are both classification algorithms, but they are used in different scenarios and have key conceptual differences:\n",
        "\n",
        "🔹 Logistic Regression Type: Binary classification\n",
        "\n",
        "Use case: When there are only two classes (e.g., spam vs. not spam).\n",
        "\n",
        "Output: A single probability that the input belongs to the \"positive\" class (usually class 1). The probability of the other class (class 0) is simply 1 − 𝑝 1−p.\n",
        "\n",
        "Activation function: Sigmoid\n",
        "\n",
        "𝜎 ( 𝑧 ) = 1 1 + 𝑒 − 𝑧 σ(z)= 1+e −z\n",
        "\n",
        "1​\n",
        "\n",
        "Decision rule: Classify as 1 if 𝜎 ( 𝑧 )\n",
        "\n",
        "0.5 σ(z)>0.5, else 0.\n",
        "\n",
        "🔹 Softmax Regression (also called Multinomial Logistic Regression) Type: Multiclass classification\n",
        "\n",
        "Use case: When there are three or more classes (e.g., classifying types of animals: cat, dog, bird).\n",
        "\n",
        "Output: A vector of probabilities, one for each class. All probabilities sum to1.\n",
        "\n",
        "###19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "Choosing between One-vs-Rest (OvR) and Softmax for multiclass classification depends on several factors, including the nature of your data, model complexity, training time, and interpretability. Here's a breakdown to help you decide:\n",
        "\n",
        "2. When to Choose OvR\n",
        "✅ Simple or Imbalanced Datasets: OvR can be more stable when classes are imbalanced.\n",
        "\n",
        "✅ Modular Design: If you want independent models per class (e.g., easier to debug or deploy separately).\n",
        "\n",
        "✅ Binary-focused Algorithms: When using models that only support binary classification natively (e.g., SVMs, Logistic Regression without softmax).\n",
        "\n",
        "🧪 Pros:\n",
        "Easier to implement with binary classifiers.\n",
        "\n",
        "More flexible if you want to focus on a subset of classes.\n",
        "\n",
        "⚠️ Cons:\n",
        "Doesn’t model inter-class relationships directly.\n",
        "\n",
        "Can be inconsistent if multiple classifiers give high scores.\n",
        "\n",
        "🔹 3. When to Choose Softmax\n",
        "✅ Mutually Exclusive Classes: When exactly one class should be predicted (e.g., classification tasks like digit recognition).\n",
        "\n",
        "✅ Probabilistic Interpretation: Softmax gives proper probability distribution over all classes.\n",
        "\n",
        "✅ Deep Learning Models: Standard choice in neural networks (e.g., image classification with CNNs).\n",
        "\n",
        "🧪 Pros:\n",
        "Jointly models all classes → often better accuracy.\n",
        "\n",
        "Single cohesive model.\n",
        "\n",
        "Probabilities sum to 1 — intuitive interpretation.\n",
        "\n",
        "⚠️ Cons:\n",
        "Might struggle with class imbalance unless handled properly.\n",
        "\n",
        "Less interpretable if you want per-class model insights.\n",
        "\n",
        "🔹 4. Performance and Scalability\n",
        "OvR: Scales linearly with the number of classes (more models to train).\n",
        "\n",
        "Softmax: Single model, but can be more computationally intensive per iteration.\n",
        "\n",
        "###20. How do we interpret coefficients in Logistic Regression?\n",
        "In Logistic Regression, the coefficients represent the log-odds of the outcome variable, not direct changes in probabilities. Here's how to interpret them:\n",
        "\n",
        "🔢 1. Log-Odds Interpretation Each coefficient 𝛽 𝑗 β j​represents the change in the log-odds of the outcome for a one-unit increase in the predictor 𝑥 𝑗 x j​, holding all other variables constant.\n",
        "\n",
        "logit ( 𝑃 ) = log ⁡ ( 𝑃 1 − 𝑃 ) = 𝛽 0 + 𝛽 1 𝑥 1 + ⋯ + 𝛽 𝑗 𝑥 𝑗 logit(P)=log( 1−P P​)=β 0​+β 1​x 1​+⋯+β j​x j​\n",
        "\n",
        "So, if 𝛽 𝑗 = 0.5 β j​=0.5, it means that a 1-unit increase in 𝑥 𝑗 x j​increases the log-odds of the outcome by 0.5.\n",
        "\n",
        "🔁 2. Odds Ratio Interpretation Exponentiate the coefficient to interpret it in terms of odds ratios:\n",
        "\n",
        "Odds Ratio = 𝑒 𝛽 𝑗 Odds Ratio=e β j​\n",
        "\n",
        "If 𝛽 𝑗 = 0.5 β j​=0.5, then 𝑒 0.5 ≈ 1.65 e 0.5 ≈1.65: the odds of the event happening are 1.65 times higher for a one-unit increase in 𝑥 𝑗 x j​.\n",
        "\n",
        "If 𝛽 𝑗 = − 0.7 β j​=−0.7, then 𝑒 − 0.7 ≈ 0.50 e −0.7 ≈0.50: the odds are halved for each unit increase in 𝑥 𝑗 x j​.\n",
        "\n",
        "🔢 3. Probability Interpretation (More Complex) The effect on probability depends on the values of all predictors, since the logistic function is nonlinear:\n",
        "\n",
        "𝑃 = 1 1 + 𝑒 − ( 𝛽 0 + 𝛽 1 𝑥 1 + ⋯ + 𝛽 𝑗 𝑥 𝑗 ) P= 1+e −(β 0​+β 1​x 1​+⋯+β j​x j​)\n",
        "\n",
        "1​\n",
        "\n",
        "So, while the log-odds and odds ratio interpretations are fixed per unit change, the actual change in probability varies depending on where you are on the curve.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "68Z8Aw9r_R5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#*Pratical Answer⭐"
      ],
      "metadata": {
        "id": "OoZApldmjOCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy?"
      ],
      "metadata": {
        "id": "I06ttXd-jV4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otu7nF6ckH8X",
        "outputId": "b2123b7f-cd75-43ff-f805-b3605aa53db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')and print the model accuracy?\n"
      ],
      "metadata": {
        "id": "Enpbx3Q8kaDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0, random_state=42)\n",
        "\n",
        "# Fit model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 regularization: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkitappQkjQj",
        "outputId": "4ffa7bdc-61a5-4acc-f6e0-ca54cea0a650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 regularization: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients?\n"
      ],
      "metadata": {
        "id": "BxewXvlmkyIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris dataset for example)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For binary classification, let's use only two classes (0 and 1)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='liblinear')  # 'liblinear' supports binary classification\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Model Coefficients:\", model.coef_)\n",
        "print(\"Model Intercept:\", model.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRiv_9Nwk5qz",
        "outputId": "3d2f341e-74a8-4750-80a9-4b65340308c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Model Coefficients: [[ 0.76824459 -1.09795391  1.45561898  1.47453658]]\n",
            "Model Intercept: [0.24425449]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')?"
      ],
      "metadata": {
        "id": "QvavlpcOlGwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define and train the logistic regression model with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',     # Elastic net penalty\n",
        "    solver='saga',            # Required solver for elasticnet\n",
        "    l1_ratio=0.5,             # Mix between L1 and L2 (0 = L2, 1 = L1)\n",
        "    C=1.0,                    # Regularization strength (inverse of lambda)\n",
        "    max_iter=10000,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tluJ6qqlRz-",
        "outputId": "3bf86fd0-b2c6-4302-80fd-c8b15d26a39a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9736842105263158\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.95      0.96        43\n",
            "           1       0.97      0.99      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.97      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'C?\n"
      ],
      "metadata": {
        "id": "RUxCJPDilWy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load dataset (Iris dataset is a common multiclass dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create logistic regression model with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wmuwD7xlfcZ",
        "outputId": "b962a62f-c8c1-4a0d-c3c3-2a22c05fa3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9777777777777777\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        19\n",
            "  versicolor       1.00      0.92      0.96        13\n",
            "   virginica       0.93      1.00      0.96        13\n",
            "\n",
            "    accuracy                           0.98        45\n",
            "   macro avg       0.98      0.97      0.97        45\n",
            "weighted avg       0.98      0.98      0.98        45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6.Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy?\n"
      ],
      "metadata": {
        "id": "6ZF_xDtal1JL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "\n",
        "# Define hyperparameters grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the model\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and accuracy\n",
        "best_params = grid.best_params_\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output the results\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Test Set Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqfo1BiKmBSx",
        "outputId": "12bacbff-f12e-4a7e-a568-ce52102e89de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
            "Test Set Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy?\n"
      ],
      "metadata": {
        "id": "BQwTS39TmRAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Define Stratified K-Fold cross-validator\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize logistic regression model\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Store accuracies\n",
        "accuracies = []\n",
        "\n",
        "# Perform Stratified K-Fold Cross-Validation\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Fold accuracy: {acc:.4f}\")\n",
        "\n",
        "# Print average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f\"\\nAverage accuracy: {average_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR2Y6YBUmaj-",
        "outputId": "4941777f-df18-43c4-e427-14edc246730f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold accuracy: 0.9649\n",
            "Fold accuracy: 0.9211\n",
            "Fold accuracy: 0.9649\n",
            "Fold accuracy: 0.9474\n",
            "Fold accuracy: 0.9735\n",
            "\n",
            "Average accuracy: 0.9543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy?\n"
      ],
      "metadata": {
        "id": "bE6u5Gs_mquS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV\n",
        "file_path = 'your_dataset.csv'  # Replace with your actual CSV file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Preview the data\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Separate features and target\n",
        "# Replace 'target_column' with the actual column name of your labels\n",
        "X = data.drop('target_column', axis=1)\n",
        "y = data['target_column']\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy of Logistic Regression: {accuracy:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "jlCljEJym7dv",
        "outputId": "6c72b86b-6f27-4347-bbe5-b7f5d54252f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-affcad5aa837>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load dataset from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'your_dataset.csv'\u001b[0m  \u001b[0;31m# Replace with your actual CSV file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Preview the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###9.Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy?\n"
      ],
      "metadata": {
        "id": "Qv7RudSsnGOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define logistic regression model\n",
        "log_reg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Define hyperparameter space\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg', 'sag']\n",
        "}\n",
        "\n",
        "# Randomized search with cross-validation\n",
        "random_search = RandomizedSearchCV(\n",
        "    log_reg,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    error_score='raise'\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "try:\n",
        "    random_search.fit(X_train, y_train)\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    # Predict and evaluate\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(\"Best Parameters:\", random_search.best_params_)\n",
        "    print(\"Accuracy on test data:\", accuracy)\n",
        "\n",
        "except ValueError as e:\n",
        "    print(\"Error during fitting:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHNTskAInRXr",
        "outputId": "f0e3cb9a-53e4-438b-b241-4160e65a5297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during fitting: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy?\n"
      ],
      "metadata": {
        "id": "e-8UQ3MDnzQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "class_labels = np.unique(y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# One-vs-One Classifier Training\n",
        "pairwise_classifiers = {}\n",
        "pairs = list(combinations(class_labels, 2))\n",
        "\n",
        "for (class1, class2) in pairs:\n",
        "    # Extract binary classification data for the current class pair\n",
        "    idx = np.where((y_train == class1) | (y_train == class2))\n",
        "    X_pair = X_train[idx]\n",
        "    y_pair = y_train[idx]\n",
        "\n",
        "    # Relabel to 0 and 1 for logistic regression\n",
        "    y_binary = (y_pair == class2).astype(int)\n",
        "\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(X_pair, y_binary)\n",
        "    pairwise_classifiers[(class1, class2)] = clf\n",
        "\n",
        "# Prediction via majority voting\n",
        "def ovo_predict(X):\n",
        "    votes = []\n",
        "    for (class1, class2), clf in pairwise_classifiers.items():\n",
        "        pred = clf.predict(X)\n",
        "        # Convert back to original class labels\n",
        "        pred_labels = np.where(pred == 0, class1, class2)\n",
        "        votes.append(pred_labels)\n",
        "\n",
        "    # Transpose and perform majority voting\n",
        "    votes = np.array(votes).T\n",
        "    final_predictions = [Counter(row).most_common(1)[0][0] for row in votes]\n",
        "    return np.array(final_predictions)\n",
        "\n",
        "# Make predictions and evaluate accuracy\n",
        "y_pred = ovo_predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7onqaF8oI6D",
        "outputId": "c99d2351-5f67-4e06-f919-9793bfaa0e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification?\n"
      ],
      "metadata": {
        "id": "p9fMAq0GoXe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Step 1: Generate a binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,    # number of samples\n",
        "    n_features=10,     # number of features\n",
        "    n_informative=5,   # informative features\n",
        "    n_redundant=0,\n",
        "    n_classes=2,       # binary classification\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize confusion matrix\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('True label')\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "fwM6p_sCogPl",
        "outputId": "991b9c37-a199-4ab1-b43e-3950951466a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,Recall, and F1-ScoreM?\n"
      ],
      "metadata": {
        "id": "cDw15w4TpDQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate a sample binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpmTzGkNpLTr",
        "outputId": "ffa41632-8859-4e5d-8fbc-a61796cac8bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9149\n",
            "Recall:    0.8037\n",
            "F1-Score:  0.8557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance?\n"
      ],
      "metadata": {
        "id": "6Au-jZCgplsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Create imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=2,\n",
        "    n_redundant=10,\n",
        "    n_clusters_per_class=1,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1\n",
        "    flip_y=0,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "print(f\"Original class distribution: {Counter(y)}\")\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train logistic regression without class weights\n",
        "model_no_weights = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"Classification report without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Train logistic regression with class weights\n",
        "class_weights = {0: 1, 1: 9}  # Manually set higher weight for minority class\n",
        "model_weights = LogisticRegression(class_weight=class_weights, max_iter=1000, random_state=42)\n",
        "model_weights.fit(X_train, y_train)\n",
        "y_pred_weights = model_weights.predict(X_test)\n",
        "\n",
        "print(\"Classification report with class weights:\")\n",
        "print(classification_report(y_test, y_pred_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-q8nzyHp1a0",
        "outputId": "9c9f0e9f-43b8-4b97-96a6-c61b3b0e60e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution: Counter({np.int64(0): 900, np.int64(1): 100})\n",
            "Classification report without class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       276\n",
            "           1       0.95      0.83      0.89        24\n",
            "\n",
            "    accuracy                           0.98       300\n",
            "   macro avg       0.97      0.91      0.94       300\n",
            "weighted avg       0.98      0.98      0.98       300\n",
            "\n",
            "Classification report with class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.97      0.98       276\n",
            "           1       0.72      0.88      0.79        24\n",
            "\n",
            "    accuracy                           0.96       300\n",
            "   macro avg       0.86      0.92      0.89       300\n",
            "weighted avg       0.97      0.96      0.96       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance?\n"
      ],
      "metadata": {
        "id": "YUcUohhIp9zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load Titanic dataset from seaborn\n",
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Inspect the first few rows\n",
        "print(titanic.head())\n",
        "\n",
        "# Select features and target variable\n",
        "# We'll predict 'survived' using some features\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "target = 'survived'\n",
        "\n",
        "# Handle missing values\n",
        "# 'age' and 'embarked' have missing values\n",
        "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
        "titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# Encode categorical variables\n",
        "# 'sex' and 'embarked' are categorical\n",
        "titanic['sex'] = LabelEncoder().fit_transform(titanic['sex'])\n",
        "titanic['embarked'] = LabelEncoder().fit_transform(titanic['embarked'])\n",
        "\n",
        "# Prepare feature matrix X and target vector y\n",
        "X = titanic[features]\n",
        "y = titanic[target]\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize and train logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3_YFcxYqJ-B",
        "outputId": "519c5943-5c1d-44f0-f884-c0cafda100a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
            "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
            "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
            "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
            "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
            "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
            "\n",
            "     who  adult_male deck  embark_town alive  alone  \n",
            "0    man        True  NaN  Southampton    no  False  \n",
            "1  woman       False    C    Cherbourg   yes  False  \n",
            "2  woman       False  NaN  Southampton   yes   True  \n",
            "3  woman       False    C  Southampton   yes  False  \n",
            "4    man        True  NaN  Southampton    no   True  \n",
            "Accuracy: 0.8044692737430168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-2067a69add35>:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
            "<ipython-input-14-2067a69add35>:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  titanic['embarked'].fillna(titanic['embarked'].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.89      0.85       110\n",
            "           1       0.79      0.67      0.72        69\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.78      0.79       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling?\n"
      ],
      "metadata": {
        "id": "_ECd8RY9q9Fz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load example dataset (Iris)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --------- Without scaling ---------\n",
        "model_no_scaling = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# --------- With Standardization scaling ---------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with standardization: {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a14cWZC_rOgB",
        "outputId": "a88fe94d-1c1e-4572-e5f0-cf05100b1dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 1.0000\n",
            "Accuracy with standardization: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score?"
      ],
      "metadata": {
        "id": "9KmMv1QvrYBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate a binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=2, n_redundant=10,\n",
        "                           random_state=42)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(f'ROC-AUC Score: {roc_auc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYTgfrtcrjY1",
        "outputId": "82bd95c0-71e6-407e-98a8-548b979079a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy?\n"
      ],
      "metadata": {
        "id": "SbhzvOaZrmIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Logistic Regression model with custom C value\n",
        "model = LogisticRegression(C=0.5, max_iter=1000, solver='lbfgs', multi_class='auto')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8HkbrbBrw4q",
        "outputId": "d852274f-4b6f-4cca-da92-bb59a363ffa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###18.Write a Python program to train Logistic Regression and identify important features based on model coefficients?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ejgpg60rr_er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get feature coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame for features and their coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient value to get importance\n",
        "feature_importance['AbsCoefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='AbsCoefficient', ascending=False)\n",
        "\n",
        "# Print important features\n",
        "print(\"Important features based on Logistic Regression coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']])\n",
        "\n",
        "# Optional: Evaluate model accuracy\n",
        "accuracy = model.score(X_test_scaled, y_test)\n",
        "print(f\"\\nModel accuracy on test set: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kqmd7NCsHGG",
        "outputId": "dc00e405-4e0f-4439-d7e0-8e0de01f1e74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Important features based on Logistic Regression coefficients:\n",
            "                    Feature  Coefficient\n",
            "21            worst texture    -1.350606\n",
            "10             radius error    -1.268178\n",
            "28           worst symmetry    -1.208200\n",
            "7       mean concave points    -1.119804\n",
            "26          worst concavity    -0.943053\n",
            "13               area error    -0.907186\n",
            "20             worst radius    -0.879840\n",
            "23               worst area    -0.841846\n",
            "6            mean concavity    -0.801458\n",
            "27     worst concave points    -0.778217\n",
            "15        compactness error     0.682491\n",
            "19  fractal dimension error     0.616230\n",
            "12          perimeter error    -0.610583\n",
            "22          worst perimeter    -0.589453\n",
            "24         worst smoothness    -0.544170\n",
            "5          mean compactness     0.540164\n",
            "18           symmetry error     0.500425\n",
            "3                 mean area    -0.465210\n",
            "0               mean radius    -0.431904\n",
            "2            mean perimeter    -0.393432\n",
            "1              mean texture    -0.387326\n",
            "14         smoothness error    -0.313307\n",
            "17     concave points error    -0.311300\n",
            "8             mean symmetry     0.236119\n",
            "11            texture error     0.188877\n",
            "16          concavity error     0.175275\n",
            "29  worst fractal dimension    -0.157414\n",
            "9    mean fractal dimension     0.075921\n",
            "4           mean smoothness    -0.071667\n",
            "25        worst compactness     0.016110\n",
            "\n",
            "Model accuracy on test set: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###19.Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score?\n"
      ],
      "metadata": {
        "id": "gX9erFGJsUv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load sample data (Iris dataset)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, convert to binary classification (class 0 vs rest)\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijh4a6LssbvW",
        "outputId": "00906865-a95b-4f16-82a1-1ce971da466e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###20.Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification?\n"
      ],
      "metadata": {
        "id": "Fcsfe7yysmlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=2, n_redundant=10,\n",
        "                           n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression\n",
        "model = LogisticRegression(solver='liblinear')  # liblinear good for small datasets\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Compute average precision score\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Logistic Regression (AP = {avg_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "bE9qTSyPs8XR",
        "outputId": "815081c8-d206-4823-c727-b9107895db5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy?\n"
      ],
      "metadata": {
        "id": "iP1jz86QtU-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define solvers to test\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store accuracy results\n",
        "accuracy_results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # Some solvers only support certain multi-class options\n",
        "    # liblinear only supports 'ovr' multi-class, others support 'auto' (which defaults to 'ovr' or 'multinomial')\n",
        "    multi_class = 'ovr' if solver == 'liblinear' else 'auto'\n",
        "\n",
        "    # Create and train Logistic Regression model\n",
        "    model = LogisticRegression(solver=solver, max_iter=5000, multi_class=multi_class, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = acc\n",
        "\n",
        "# Print accuracy results\n",
        "for solver, acc in accuracy_results.items():\n",
        "    print(f\"Solver: {solver:8s} --> Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQtAYVz_tdMh",
        "outputId": "093444fe-2a3e-419a-afc6-5291f054df06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear --> Accuracy: 0.9778\n",
            "Solver: saga     --> Accuracy: 1.0000\n",
            "Solver: lbfgs    --> Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###22.Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)?\n"
      ],
      "metadata": {
        "id": "A4wYnf2Utp6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=2, n_redundant=10,\n",
        "                           random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.3,\n",
        "                                                    random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nHPoccQtzpe",
        "outputId": "d4b7f98b-1a78-4ba1-ac7f-d4d465e22271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.6795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###23.Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling?\n"
      ],
      "metadata": {
        "id": "2pG3UDJCt-DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# 2. Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on raw data: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZE9y98buFsY",
        "outputId": "90158231-7907-4806-e09a-e69825b7f0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 1.0000\n",
            "Accuracy on standardized data: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###24.Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation?\n"
      ],
      "metadata": {
        "id": "KMG5jMphuULD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model\n",
        "log_reg = LogisticRegression(max_iter=1000, solver='liblinear')  # 'liblinear' works well for small datasets\n",
        "\n",
        "# Define the grid of C values to search\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
        "\n",
        "# Set up GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train models with cross-validation to find the best C\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best regularization parameter\n",
        "print(f\"Best C: {grid_search.best_params_['C']}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1kvdmcDubdb",
        "outputId": "40b42ff8-82d5-420c-ce5c-64b00e2fe831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C: 10\n",
            "Test set accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n"
      ],
      "metadata": {
        "id": "-zP9FzP6ur15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "# 1. Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Save the trained model to a file\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "# 5. Load the model from the file\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# 6. Make predictions on test data\n",
        "predictions = loaded_model.predict(X_test)\n",
        "print(\"Predictions:\", predictions)\n",
        "\n",
        "# Optional: Evaluate accuracy\n",
        "accuracy = loaded_model.score(X_test, y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6WK60tZuye0",
        "outputId": "f74f8ed9-8cb0-405e-8cda-e972adfe62fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully.\n",
            "Model loaded successfully.\n",
            "Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Accuracy: 1.00\n"
          ]
        }
      ]
    }
  ]
}